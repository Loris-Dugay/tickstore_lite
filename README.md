# Project 1 â€” TickStore Lite (SQUELETTE) ğŸ§±

Ce dÃ©pÃ´t est **un squelette vide** pour rÃ©aliser le projet. Aucun code nâ€™est fourni, seulement la structure,
les contraintes, le barÃ¨me et les modalitÃ©s de rendu. **Ã€ toi dâ€™implÃ©menter.**

---

## Objectif
Construire un mini **tick data lake** local :
- Ingestion de **trades** (et, optionnellement, **quotes**) depuis CSV.
- Ã‰criture en **Parquet partitionnÃ©** (`date=YYYY-MM-DD/symbol=SYMB`).
- Calcul de **barres 1s** (OHLC, volume, **VWAP**).
- ExÃ©cution de **requÃªtes SQL** (DuckDB **ou** ClickHouse).
- (Bonus) Jobs **Spark** Ã©quivalents batch (et streaming si tu veux).

---

## Ã€ utiliser **absolument**
1. **Python 3.10+** et **SQL** (DuckDB **ou** ClickHouse â€” au moins lâ€™un des deux).
2. **Parquet** avec **partitionnement** par **date** et **symbol**.
3. Une **CLI Python** (Typer ou argparse) exposant au minimum :
   - `ingest` : CSV â†’ Parquet
   - `bars` : Parquet â†’ barres 1s
   - `checks` : contrÃ´les de base (non-vide, nulls critiques, etc.)
4. Un **Makefile** avec ces cibles (noms imposÃ©s) :
   - `make setup` (crÃ©ation venv + install deps)
   - `make ingest`
   - `make bars`
   - `make sql-queries` (exÃ©cute 5 requÃªtes types et Ã©crit des rÃ©sultats lisibles)
   - `make test` (pytest)
   - `make grade` (rapport JSON/TXT dans `out/` â€” tu dÃ©finis la logique)
5. **Tests PyTest** (au moins 3), couvrant ingestion + barres.
6. **README clair** expliquant comment lancer chaque Ã©tape.
7. **Gestion de lâ€™horodatage** : `ts` en UTC (ou traitÃ© comme UTC) **sans mÃ©lange de timezones**.

### SchÃ©mas **minimaux** attendus
- **Trades** : `ts` (ISO8601), `symbol` (str), `price` (float), `size` (int), `exchange` (str)
- **Quotes (optionnel)** : `ts`, `symbol`, `bid`, `bid_size`, `ask`, `ask_size`, `exchange`

---

## Interdit / Non comptabilisÃ©
- âŒ **Sorties finales uniquement en CSV** (le Parquet partitionnÃ© est obligatoire).
- âŒ **Notebooks** comme **unique** pipeline (ok pour lâ€™exploration, mais la CLI est requise).
- âŒ **C++/Rust** pour ce projet (on reste **Python/SQL** ; Spark en bonus).
- âŒ DÃ©pendances cloud payantes ou services nÃ©cessitant des clÃ©s privÃ©es.
- âŒ Commits dâ€™artÃ©facts lourds (>50â€¯MB) dans le repo (donnÃ©es dâ€™exemple lÃ©gÃ¨res ok).
- âŒ Modifier le barÃ¨me ci-dessous dans ton rendu (tu peux proposer un barÃ¨me *bonus* sÃ©parÃ©).

---

## Contraintes supplÃ©mentaires
- **Idempotence** : relancer `make ingest`/`make bars` ne doit pas casser le lake (Ã©critures dÃ©terministes).
- **ReproductibilitÃ©** : tout doit se lancer depuis zÃ©ro avec les cibles Make.
- **Performance indicative (local)** : le dataset dâ€™exemple doit sâ€™exÃ©cuter en < 10â€¯s.

---

## BarÃ¨me (100 pts)
- **Ingestion & partitionnement** (20 pts)
- **Barres 1s exactes (OHLC, volume, VWAP)** (30 pts)
- **RequÃªtes SQL (5 mini)** (10 pts)
- **QualitÃ© & idempotence (checks, re-runs OK)** (15 pts)
- **Automatisation & clartÃ© (Makefile/README/tests)** (15 pts)
- **(Bonus) Spark track** (10 pts)

> La note finale inclut une courte revue de code (lisibilitÃ©, structure, docstrings, logs).

---

## ModalitÃ©s de rendu
**Option A â€” GitHub (prÃ©fÃ©rÃ©)**  
- Repo public nommÃ© `tickstore-lite-<ton_pseudo>`
- Inclure ce README (complÃ©tÃ©), le **Makefile** et les scripts.
- Tag `v1.0` quand câ€™est prÃªt.

**Option B â€” Archive**  
- Envoyer un `.zip` du dossier (sans venv), avec `data/sample/` **lÃ©ger** pour reproduire.

### Ce que je lancerai pour te noter
```bash
make setup
make ingest
make bars
make sql-queries
make test
make grade
```
- Les rÃ©sultats attendus :  
  - Parquet sous `data/lake/` et `data/derived/bars_1s/` (partitionnÃ© `date, symbol`).
  - `out/grade_report.json` + `out/grade_report.txt` avec mÃ©triques clÃ©s (lignes ingÃ©rÃ©es, nb de barres, timings, checks OK/KO).

---

## Structure fournie (Ã  complÃ©ter)
```
.
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt               # Ã  complÃ©ter par toi
â”œâ”€â”€ requirements-spark.txt         # optionnel (Spark)
â”œâ”€â”€ pyproject.toml                 # metadata outillage (peut rester minimal)
â”œâ”€â”€ Makefile                       # cibles imposÃ©es (TODO)
â”œâ”€â”€ conf/
â”‚   â””â”€â”€ example_config.yaml        # exemple de config (TODO)
â”œâ”€â”€ src/
â”‚   â””â”€â”€ tickstore/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ cli.py                 # CLI (vide)
â”‚       â”œâ”€â”€ ingest.py              # ingestion (vide)
â”‚       â”œâ”€â”€ compute_bars.py        # barres 1s (vide)
â”‚       â””â”€â”€ quality_checks.py      # checks (vide)
â”œâ”€â”€ jobs/
â”‚   â””â”€â”€ spark/
â”‚       â”œâ”€â”€ batch_ingest.py        # (vide)
â”‚       â””â”€â”€ compute_bars_spark.py  # (vide)
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_sample.py             # (vide) â€” Ã  remplir
â”œâ”€â”€ tools/
â”‚   â””â”€â”€ grade.py                   # (vide) â€” Ã  dÃ©finir par toi
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ sample/                    # mets quelques CSV lÃ©gers ici
â”‚   â”œâ”€â”€ lake/                      # sorties Parquet (ingestion)
â”‚   â””â”€â”€ derived/
â”‚       â””â”€â”€ bars_1s/               # sorties Parquet (barres)
â””â”€â”€ .github/workflows/ci.yml       # pytest basique (TODO)
```

---

## Conseils
- Commence par un petit dataset (2â€“3 symboles, quelques secondes).  
- Valide dâ€™abord en **DuckDB** (plus simple), puis ajoute **ClickHouse** si tu veux.  
- Garde les **logs structurÃ©s** (niveau INFO), et un `--dry-run` utile.  
- La piste **Spark** est un **bonus** (montre batch dâ€™abord).

Bon courage ! ğŸš€
